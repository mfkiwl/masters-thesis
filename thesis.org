# -*- eval: (visual-line-mode t) -*-
#+TITLE: RTIC Scope — Real-Time Tracing Support for the RTIC RTOS Framework
#+AUTHOR: Viktor Sonesten
#+EMAIL: vikson-6@student.ltu.se
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [twocolumn]
#+options: toc:nil
#+latex_header: \usepackage{libertine}
#+latex_header: \usepackage{inconsolata}
#+latex_header: \usepackage[citestyle=authoryear-icomp,bibstyle=authoryear, hyperref=true,maxcitenames=3,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \addbibresource{ref.bib}
#+latex_header: \usepackage{microtype}

* Org setup
  #+begin_src emacs-lisp :result output :session :exports both
    (require 'ox-extra)
    (ox-extras-activate '(ignore-headlines))
  #+end_src

* *The Paper*                                                        :ignore:

# Make this a single paragraph; use unambiguous terms; aim for 250 words; 3-5 keywords.
#+begin_abstract
Here be an abstract...
#+end_abstract

** TODO Introduction
# What are embedded systems, regulators, and how do they relate?
Embedded systems --- a unit combination of a central processing unit, system memory, and input/output peripheral devices --- serve a key role in the operation of systems with electronical components where computations must be made.
Examples of such systems are digital control systems which regulate one or more control quantities such that they adhere to within derigned characteristics.
Often the goal is to track a reference signal.
Such a regulator observes the internal states of a system under control via sensors and affect the system via actuators.
For example, one may wish to keep a rocket on a set path to reach orbit, or control the internals of a nuclear power plant to maximize electrical power output and keep the reactor in a safe state.

# On the real-time restrictions of control systems; exponential complexity phenomena.
A key design parameter of digital controllers is the sample rate: if too low or unconsistent, the controlled system will end up unstable. cite:franklin
The rocket may thus fly off-course, or the nuclear reactor reach a meltdown.
This puts a real-time constrain on the digital controller which greatly limits how it can be implemented.
Further, a digital controller under implementation must be debugged so that the engineers can verify its correct operation, which grows exponentially in difficulty with the number of tasks that must be done[fn:2].
The same exponential phenomena is observed for dynamic systems with a growing number of states, inputs, and outputs.

# The observer effect; data exfiltration.
Unfortunately, embedded systems are subject to the observer effect: to observe the internal state of a embedded system (i.e., system variables in memory) its operation must be affected.
# A proper implementation would not block on a serial write.
This observeration is often realized by exfiltrating data via serial communication which in the best case is subject to bus contention and full output queues.
In a perfect implementation, the user application[fn:1] would only concern itself with its mainsake prodecure and leave data exfiltration to a completely disjoint system, thus minimizing the observer effect and thus the effect on the underlying control system.

# ARM, tracing subsystem and possible exploitation.
ARM is an ubiquitous vendor for embedded systems with a number of sub-vendors (e.g. STMicroelectrics, NXP Semiconductors, Nordic Semiconductors).
The ARM Cortex-M7 standard offers debugging facilities that enables the developer to trace the system.
From ARM's documentation, "Trace refers to the process of capturing data that illustrates how the components in a design are operating, executing, and performing". cite:arm-cortex-learn
Additionally, "[Trace generation is] almost entirely non-invasive. [Trace generation] does not influence the wider system".
This trace facility allows an event-based monitoring of
- interrupt enters and exits (tracing of hardware tasks, e.g. the main control loop);
- write access to in-memory adress spaces (system state variables, software tasks);
- comparator matches, among other features.
The generated trace is then exfiltrated from via serial using a packet protocol, henceforth referred to as the "trace stream".
This trace stream (with system-external collection) is thus a suitable candidate for exploitation to realize a control system with minimal observer effect.

# Less work, more sleep.
Further, with non-invasive trace generation, minimal code must be executed in the user application, allowing a shorter duty-cycle of the controlling program.
With a decreased duty-cycle, the system can be put to sleep longer, and thus conserve energy.

# On real-time implementation restictions, embedded implementation difficulties in general. Enter RTIC.
The development of embedded system in general is a difficult one.
In comparison to general-purpose computers, where one often need not worry about resource limitations, embedded systems are constrained in all manners such that costs can be minimized for their non-general applications.
# No rich OS; no two embedded platforms are the same.
An embedded developer seldom have access to a rich operating systems (OS) --- a Linux-based distibution, for example --- which offers general-purpose facilities based upon dynamic allocations and a common environment to simplity implementation.
Embedded platforms usually differ significantly, and porting an implementation to another platform is no small task.
This compares to general-purpose computers where a program written on one computer can be executed on another.
# Side effects and priority inversions.
Embedded platforms much more subject to side-effects where peripherals are operated by writing data to memory-mapped registers.
Of certain importance is the problem of priority inversions, where a task of lower priority executes instead of a higher prioritized task because of implementation error.
In summary, it is easy to put an embedded system in an incorrect or unknown state.
# Enter RTIC.
In order to lighten the implementation burden one may employ Real-Time Interrupt-driven Concurrency (RTIC), a real-time OS (RTOS) developed at Luleå Technical University based on the extensively studied Stack Resource Policy (SRP), which mitigates priority inversions while also offering determinant scheduling of future tasks and message-passing between tasks.
# TODO refer to rauk
RTIC also lends itself to static analysis by help of external tools, further lighening the overhead of an embedded implementation.

# Project aim
The aim of this thesis is thus to employ and extend upon RTIC with a toolset to leverage ARM's non-intrusive tracing facilites whilst requiring minimal overhead for then end-user developer, greatly lightening the burden to implement digital control systems.
The resulting toolset will be employed to implement a regulator for a non-trivial dynamic system, and the ergonomics of the toolset evaluated.

# TODO throw in some stats on how much ARM is used in industy
# TODO "mission-critical systems are thus implemented in worst case scenario, doing more work than necessary, because it is easier to analyze"?

 # The vendor ARM...
 # - ITM
 # - tracing (debugging) without affecting user application.
 # - nothing need to be done from the user applicaiton.
 # - watch adresses are impl. by monitoring user application from an otherwise disjoint system (what about clock?)
 # - software tasks require user application effect, but RTIC Scope aims for minimal
 # - hardware tasks are traced via interrupts, but no effect on user application.

*** Background
**** Real-time Operating Systems (RTOS)
**** Real-Time Interrupt-driven Concurrency (RTIC)
 RTIC (cite:rtic) is a real-time operating system (RTOS) based on the stack resource policy (cite:baker90) for task scheduling written in the Rust programming language.

***** Rust
****** TODO refer to Tjäder's thesis when it comes to Rust?
***** RTIC syntax and features
**** Hardware features
***** Breakpoints and watchpoints
***** Tracing
 RTIC supports the ARM Cortex-M4 family of microprocessor core units (MCUs) which, in turn, offer asynchronous debug facilites for real-time tracing support (cite:arm-rm, §C).
 Of chief interest are
 - DWT, :: Data Watchpoint and Trace (cite:arm-rm, §C1.8): contains program counter and address comparators that signal on a match; and
 - ITM, :: Intrumentation Trace Macrocell (cite:arm-rm, §C1.7): trace information generator in the form of packets; multiplexes trace information from other sources (e.g. DWT).

 # Ref. does not say that ITM is real-time.
 Tracing is the ability to analyse the behavior of an embedded system in real-time without significantly affecting the user application, known as non-intrusive debugging.
 Proper application of tracing allows the developer to verify the behavior of an embedded system.

 ITM is realized in practise by communicating between the embedded system and the analysing system with a packet protocol. (cite:arm-rm, Appendix D4)

 # This does not fit in the background
 If the embedded system has the capability, tracing data may be buffered locally before it is transferred to an external system.
 The information can also be captured by monitoring a serial pin.
***** TODO refer to [[*Theory and Methodology]] for further details
      We only want to cover the basics here in the background.

*** Motivation
 Debugging the user application running on an MCU is an integral part of an embedded work-flow.
 Thus, the more debugging facilities that are readily available to the end-user of an RTOS, the better.
 More so if proper usage of such facilities - which are commonly non-trivial on embedded systems  - is abstracted.
 However, care must be taken when utilizing debugging features on an embedded target as it should not significantly affect the user application, lest real-time properties will differ between a debug and production environment.

 # Talk about RTIC and its increasing usage
 # We want to make it very simple for the end user to trace an application

*** Problem definition
 An auxiliary toolset for tracing RTIC applications is to be created.
 This toolset shall be able to
 1. collect raw trace data from the target device;
 2. interpret trace data;
 3. associate trace data to timestamped RTIC tasks;
 4. save the trace data for offline analysis; and
 5. present the trace to an end-user in a human-readable fashion.

*** Contributions
 The realization of such a toolset is a collection of crates that constitute the RTIC Scope project:
 - ~itm-decode~ :: a library that decodes raw trace data to a set of Rust structures for easy management,
   thereby fulfilling requirement 1 of [[*Problem definition]].
 - ~cargo-rtic-scope~ :: a cargo subcommand that acts as daemon:
   it records raw trace data, associates it to timestamped RTIC tasks by help of ~itm-decode~, saves it to file, and forwards it to any spawned frontends; thereby fulfilling requirements 2--4.
 - ~rtic-scope-frontend-dummy~ :: a reference frontend implementation that simply prints timestamped RTIC tasks to =stderr=;
   thereby fulfilling the last requirement, 5.

 From an end-user perspective RTIC Scope offers a "batteries-included" tool that enables great insight into a target RTIC applications,
 provided that a small set of limitations are adhered and specific metadata is added to the application crate in question.

 The necessary end-user actions can be summarized by the following commands:
 #+begin_src fundamental
   $ cargo install cargo-rtic-scope
   $ cargo install rtic-scope-frontend-dummy
   $ # Example metadata added to a target RTIC application in a crate named "trace-examples"
   $ cargo metadata --format-version 1 | jq '.packages[] | select(.name == "trace-examples") | .metadata'
   {
     "rtic-scope": {
       "interrupt_path": "stm32f4::stm32f401::Interrupt",
       "pac": "stm32f4",
       "pac_features": [
         "stm32f401"
       ]
     }
   }
 #+end_src

 # XXX what section? next section is thesis limitations
 See the next section on source code limitations.

*** Limitations
 The work that can be done to solve the [[*Problem definition]] is virtually endless,
 especially regarding the fifth requirement because of the many possible approaches to design an end-user facing application.
 The scope limit of this thesis is thus the implementation of a daemon that fulfills requirements 1--4,
 and a barebones implementation of a frontend for requirement 5.
 As the ~dummy~-suffix implies, this is a frontend that does limited work.
 The reason for this limitation is focus on a delivery of a robust backend that does the heavy lifting.
 The Embedded Rust community is then fully welcome to implement frontends that suit their needs.

 Further:
 - the work of this thesis will not stray far from the ITM specification.
   ETM, for example, will not be exploited.
 - Only RTIC v0.6 (currently in development) will be considered for the final release of RTIC Scope that occur within the frames of this thesis.
   Releases past those of this thesis will handle future RTIC releases.

*** TODO Previous work
  - itm-tools[fn:itm-tools] :: Some work has already been made to integrate ITM tracing in an RTIC application[fn:itm-tools], but the approach is ad-hoc and not abstracted for the user.
    Nevertheless, a base to work from is available and will be used.
  - probe-rs :: is an extensible debugging toolkit with in-development support for ITM tracing (cite:probe-rs) that fits into the ecosystem of RTIC.
    +Work will be done on this toolkit to enable a "batteries included" implementation of the problem solution.+
  - memory lanes[fn:memory-lanes] :: If it is found that more data than what ITM can provide is required for further tracing details, the usage of RTT will be investigated.[fn:memory-lanes]

*** TODO Related work
  - orbuculum :: https://github.com/orbcode/orbuculum. Probably akin to the daemon we want to create.
  - Percepio Tracealyzer :: See https://percepio.com/tracealyzer/.

  # (Probably) refer to other (proprietary) implementations

*** TODO Outline
 This paper is structured as follows
 - [[Introduction]] :: provides an introduction to Rust, RTIC, ARM hardware peripherals of interest, and the RTIC Scope project.
 - [[Previous work]] :: presents work previously done in the same domain, which this thesis and RTIC Scope builds upon.
 - [[Related work]] :: presents some tools similar to the features of RTIC Scope.
 - [[Theory]] :: covers the exploited ARM peripherals in detail, and what information is required to associate trace data to RTIC tasks.
 - [[Implementation]] ::
 - [[Results]] ::
 - [[Discussion]] ::
 - [[Conclusions]] ::
 - [[Future work]] ::
 - Appendices ::
** TODO Method
 This section describes the protocols, hardware peripherals, and software frameworks utilized in RTIC Scope for then end-goal of real-time task tracing.

 Sections [[*Instrumentation Trace Macrocell (ITM)]]--[[*Trace Port Interface Unit (TPIU)]] detail the peripherals used to generate the packet stream intercepted by RTIC Scope.
 Section [[*RTIC]] detail the RTOS which the target application is expected to be written in, and how its metadata is extracted for use in RTIC Scope.

*** Tools Used
**** Instrumentation Trace Macrocell (ITM)
  Include Fig. C1-1 from [[pdf:~/exjobb/thesis/docs/DDI0403E_d_armv7m_arm.pdf::713++0.00][DDI0403E_d_armv7m_arm.pdf: Page 713]]?
***** Decoding the ITM packet stream
  This is done with https://lib.rs/crates/itm-decode.
***** Trace collection
  # Talk about the difference between asyncronous serial (via SWO) and
  # synchronous serial communication (when another wire is used as a
  # clock).

  In practise, when using asynchronous serial communication for collecting
  trace data. It it not uncommon that the traced application must be
  restarted a few times until exepceted data is received on the host.

**** Data watchpoint and trace (DWT) units
     :PROPERTIES:
     :CUSTOM_ID: DWT
     :END:
  A data watchpoint and trace (DWT) unit is a hardware component that offers watchpoint functionality and common tracing operations.
  In this project, the watchpoint feature

***** TODO describe what breakpoints and watchpoints are?
**** Trace Port Interface Unit (TPIU)
  Acts as a bridge between ITM and the outer world.
**** RTIC
***** Hardware tasks
  Hardware tasks are regular Rust functions that are bound to a hardware interrupt.
  When this interrupt is made pending in hardware, the task function executes.
  An example hardware task is declared via
  #+name: rtic-hw-task-example
  #+begin_src rust
    #[app]
    mod app {
        #[task(bound = EXTI0)]
        fn foo(_ctx: foo::Context) {
            // ...
        }
    }
  #+end_src
  With this declaration, =foo= will be executed when ~EXTI0~ is made pending in hardware.
  After =foo= returns, the interrupt has been handled and ~EXTI0~ is no longer pending.

***** Tracing hardware tasks
  Hardware tasks are exclusively bound to singular hardware interrupts.
  Because of this, whenever an interrupt handler executes (and thus the bound hardware task), an =ExceptionTrace { exception, function }= packet is emitted, where =exception= is the exception number as an integer and =function= is the action context of the exception: an exception is either entered, exited, or returned.

***** Resolving hardware task names
  =exception= is a number from (cite:arm-rm; Table B1-4), the external interrupt subset of which is modelled by =PAC::Interrupt=.
  This =Interrupt= enum is used by RTIC.
  To associate an =ExceptionTrace= to an RTIC task one must find
  - which hardware interrupt a task is bound to; and
  - what interrupt number this hardware interrupt is associated with.

  For the first issue, as seen in [[rtic-hw-task-example]], the bound hardware interrupt is declared in the source code.
  Associating task name to hardware interrupt can thus be done by parsing the source code.
  This can be done via ~rtic-syntax~ [fn:rtic-syntax].

  Finding the hardware interrupt from the interrupt number is a more involved process, even though the information needed is readily available in =PAC::Interrupt=.
  Because Rust does not support reflection it is not possible to inspect different =PAC= types in a single executable.
  The only approach available for resolving is some =Ident -> u8= function.
  There are multiple approaches for how such a function can be implemented.
  They are below enumerated and considered:
  - Parsing the source code of the different =PAC::Interrupt= structures: such a structure can be declared via
    #+begin_src rust
      #[repr(u8)]
      enum Interrupt {
          EXTI0 = 6,
          EXTI1 = 7,
          // ...
      }
    #+end_src
    It is then possible to download the crate source and parse this structure similar to the RTIC application.
    Fortunately, as this crate is generated by ~svd2rust~ and it is in the interests of its developers to generate as simple code as possible, the right-hand side of the =Interrupt= variants are always integer literals.
    These can trivially be converted to the wanted =u8= type.
    The problem thus minimizes to finding the =enum Interrupt= structure in he crate.
    The one "clue" given us to this end is the PAC in the =device= argument in the =rtic::app= macro.
    For example, if an RTIC application is declared with =#[app(divice = stm32f4::stm32f401)]=, it is likely that the =enum Interrupt= structure can be found in some ~/stm32f4/stm32f401/mod.rs~ source file.
    Alternativly, it may also be inlined in a single source file, say ~lib.rs~:
    #+begin_src rust
      mod stm32 {
          mod stm32f401 {
              #[repr(u8)]
              enum Interrupt {
                  // ...
              }
          }
      }
    #+end_src
    The host application could support a range of PAC structures to ultimately find the =Interrupt= structure.
  - Dynamically build, load, and call an adhoc cdylib crate that exposes =[Ident -> u8]= functions: All =PAC::Interrupt= structures implement the =bare_metal::Nr= trait.
    As the name implies, it allow us to call, for example =PAC::Interrupt::EXTI0.nr()= to get the interrupt number of =EXTI0=.
    This trait can be exploited.
    For the set of bounds that is parsed from an RTIC application:
    1. Parse the value of the =rtic::app= macro =device= argument into a =first::second= structure, where =second= is optional.
       For example, if an application is declared via =#[app(device = stm32f4::stm32f401)]=, =stm32f4= is mapped to =first=; =stm32f401= to =second=.

       It is here assumed that =first= is the crate that contains the =enum Interrupt= structure;
       =second= is the required crate feature if specified; and that the =enum Interrupt= is available under =first::second::Interrupt=.
    2. Create a cdylib[fn:cdylib] crate in a temporary directory that depends on =first= with the feature =second= (if specified).
    3. For each bind, generate a function that maps the bind to its interrupt numbers. For example, if the bind is =EXTI0=, generate
       #+begin_src rust
         #[no_mangle]
         pub extern fn EXTI0() -> u8 {
             first::second::Interrupt::EXTI0.nr()
         }
       #+end_src
    4. Build the crate using ~cargo~. [fn:cargo]
    5. Dynamically load the generated shared object file.
    6. For each bind, find the associated =extern fn() -> u8= symbol from the bind name, and call the function.
    7. Collect the bind names and associated interrupt numbers into a =<Ident, u8>= map.

  With the above approaches, we would have a mapping from RTIC task names to their bound hardware interrupt, and a mapping from hardware interrupt name to the interrupt number.
  Consequently, we would have a mapping from interrupt number to RTIC task name.
  Thus, an =ExceptionTrace= can then readibly be associated with a RTIC hardware task.
  These proposed procedures must be repeated once per application and PAC crate used.
  Of course, caching can be utilized to minimize the number of repeated steps.

  While both approaches can be used for the implementation of a =Ident -> u8= function, and both depend on the underlying PAC, they depend on different PAC structure: the source parsing approach depends on the lexical structure of the PAC's source code; and the cdylib approach on the parsed structure of the crate (that is, instead of parsing the source code ourselves, we leave that task to Rust itself).
  Additionally, multiple different lexical structures can map to the same parsed structure; if ~svd2rust~ decides on a lexical change, the host application would have to be changed also.
  It is then understood that the cdylib approach presents the smallest problem when implementing our wanted =Ident -> u8=, and is thus chosen as the best approach.

***** Software tasks
  Software tasks are also regular Rust functions that are bound to hardware interrupts, but the bound hardware interrupt is not exclusively associated to the task in question: a single hardware interrupt can be associated with multiple software tasks.
  For this reason, the used hardware interrupt is considered a "dispatcher".

  An example software task is declared via
  #+begin_src rust
    #[app(dispatchers = [EXTI0])]
    mod app {
        #[task]
        fn bar(_ctx: bar::Context) {
            // ...
        }
    }
  #+end_src

  In difference to hardware tasks, software tasks can be scheduled by software.

***** Tracing software tasks
  Because the implementation of software tasks utilizes hardware interrupts, software tasks can be traced in the same manner as hardware tasks if it is ensured that every dispatcher only manages a single software task.
  However, in practise a dispatcher commonly manager multiple software tasks.
  An emitted =ExceptionTrace= thus tells us when a dispatcher starts, but not which software task it dispatches.

***** Resolving software task names
  The =ExceptionTrace= does not give us all the information we need.
  Instead, a [[#DWT]] unit can be employed to emit =DataTraceValue= packets on software task enter and exit.
  Via this approach, each software task is given a unique ID and code is injected (either by the =rtic::app= macro or by the end-user themselves) to write this unique ID at the start and end of the software task.
  The emitted =DataTraceValue= packets are then analysed by the host application, which maintains a state of which software task is currenly running.[fn:dwt-running-bit]
  The RTIC application source is then parsed to associate =DataTraceValue= payloads back to their software tasks.

  In comparison to hardware tasks, which are practically traced for free, software tasks can be traced at the cost of a few register writes and a dedicated DWT unit.

*** TODO Implementation
**** Hardware tasks
**** Software tasks

** TODO Results
*** Software task tracing overhead
 Here we can actually test what the overhead is of the two ~u32~ memory writes.
 Perhaps we can figure out the best way to store the watch address in memory too.
 We should plot the cycle count of traced software tasks when using dispatchers vs. DWT units.
** TODO Discussion
*** Tracing overhead
*** TODO Conclusions
*** TODO Future work

  \printbibliography
  \appendices

** TODO Application to a non-linear control system
 # The results of the R7014E-alike course

* Footnotes

[fn:2] Additional tasks could for example include: handling firmware updates over the air. # TODO more examples?
[fn:1] The program that executes on the embedded system when initialization has concluded. Also referred to as the "main loop".

[fn:cargo-cdylibs] See
https://docs.rs/cargo/0.52.0/cargo/core/compiler/struct.Compilation.html#structfield.cdylibs.

[fn:cdylib] A cdylib crate is a crate that specifies =crate_type = ["cdylib"]=.
Upon building the crate a dynamic library (a shared object file) that targets the stable C ABI is generated.
Additionally, it is trivial to find the file location of cdylibs with cargo[fn:cargo-cdylibs].
This is not the case with dylibs that instead target the unstable Rust ABI.
The only way to generate a shared object file is by building a dylib or a cdylib.

[fn:dwt-running-bit] Alternatively, one bit in the =DataTraceValue= payload can denote whether a task was entered or exited.

[fn:cargo] See https://crates.io/crates/cargo.

[fn:rtic-syntax] See https://crates.io/crates/rtic-syntax.

[fn:decoder] Based upon the existing works of ~itm-tools~[fn:itm-tools].

[fn:memory-lanes] https://github.com/rtic-rs/rfcs/issues/31 discusses the RTIC-abstraction of RTT and similar peripherals to "memory lanes".

[fn:itm-tools] See https://github.com/japaric/itm-tools.

[fn:cli] Command-line interface.
